---
title: "LLSIM geostat"
output: bookdown::html_document2
date: "`r Sys.Date()`"
params:
  use_sets: TRUE
  fleets: !r c(1, 2, 3)
  species: 'bum'
  spatiotemporal: "iid"
  spatial: "on"
  share_range: FALSE
  cache_path: "geostat"
  anisotropy: TRUE
---

Settings:

```{r params}
print(params)
```

# TODO

- [x] add version without spatial/spatiotemporal for comparison
- [x] add basic explanation below
- [-] figure out nice figs for main writeup
- [ ] compare to Beth's using same metrics?
- [x] maybe take the time to run the spatiotemporal all models
- [x] make projected plot not look laughable

# Conclusion notes

- Tweedie often doesn't converge here
- seems to be strong anisotropy... but think about projection
- coverage worse and metrics worse if no spatiotemporal or no fields at all
- convergence can be finicky with properties of mesh! priors would help
- exactly which family comes out on top is sensitive to the mesh
- all decent families
- just spatial gets you most of the way there
- + spatiotemporal helps a bit and mostly with appropriate CI coverage

# Analysis

```{r check-us, include=FALSE}
us_version <- identical(params$fleets, 1)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  autodep = TRUE,
  fig.width = 9,
  fig.asp = 0.618,
  out.width = "80%",
  fig.pos = "ht",
  cache.comments = TRUE,
  dev = "png",
  dpi = 150, 
  cache.path = paste0(params$cache_path, "-cache/"),
  fig.path = paste0(params$cache_path, "-files/")
  # optipng = "-strip all"
)
# knitr::knit_hooks$set(optipng = knitr::hook_optipng)
```

```{r libs}
library(dplyr)
library(ggplot2)
library(sdmTMB)
theme_set(theme_light())
```

```{r read-dat, include=FALSE}
dir.create("data-generated", showWarnings = FALSE)
if (params$use_sets) {
  dobs <- readr::read_csv("data-raw/obsset05.csv")
  dlog <- readr::read_csv("data-raw/logset05.csv")
} else {
  dobs <- readr::read_csv("data-raw/obstrip05.csv")
  dlog <- readr::read_csv("data-raw/logtrip05.csv")
}
names(dobs) <- tolower(names(dobs))
names(dlog) <- tolower(names(dlog))

if (params$use_sets) {
  dobs$bum <- dobs$c.bum
  dobs$swo <- dobs$c.swo
  dlog$bum <- dlog$c.bum
  dlog$swo <- dlog$c.swo
}

dobs$bycatch <- dobs[[params$species]]
dlog$bycatch <- dlog[[params$species]]
```

```{r filter-fleet, include=FALSE}
dobs <- filter(dobs, fleet %in% params$fleets)
dlog <- filter(dlog, fleet %in% params$fleets)
```

The dataset we are starting with:

```{r print-data}
select(dobs, year, month, lat, lon, gear, light, bait, fleet, hbf, hooks, season, bum) |> 
  glimpse()
```

```{r filter-us, include=FALSE}
# Explain large spatial outliers
# if (us_version) {
#   dobs <- filter(dobs, lon < 0, lat > -20)
#   dlog <- filter(dlog, lon < 0, lat > -20)
# }
```

<!-- <https://desktop.arcgis.com/en/arcmap/latest/map/projections/equidistant-conic.htm> -->
<!-- <https://en.wikipedia.org/wiki/Equidistant_conic_projection> -->

We will use a custom Albers equal-area projection with standard parallels set up to be appropriate for these data.
These types of models are often fit with UTMs, but UTMs are likely not ideal given the data span many UTM zones.
Future work could evaluate the sensitivity to the projection used.

```{r project}
if (us_version) { # US only
  conic <- paste0(
    "+proj=eqdc +lat_0=0 +lon_0=-20 +lat_1=-10 ",
    "+lat_2=40 +x_0=0 +y_0=0 +datum=WGS84 +units=km +no_defs"
  )
} else {
  conic <- paste0(
    "+proj=eqdc +lat_0=0 +lon_0=-20 +lat_1=-30 ",
    "+lat_2=35 +x_0=0 +y_0=0 +datum=WGS84 +units=km +no_defs"
  )
}
if (us_version) { # US only
  albers <- paste0(
    "+proj=aea +lat_0=0 +lon_0=-20 +lat_1=-10 ",
    "+lat_2=40 +x_0=0 +y_0=0 +datum=NAD83 +units=km +no_defs"
  )
} else {
  albers <- paste0(
    "+proj=aea +lat_0=0 +lon_0=-20 +lat_1=-30 ",
    "+lat_2=35 +x_0=0 +y_0=0 +datum=NAD83 +units=km +no_defs"
  )
}

project_dat <- function(x, crs, mult) {
  x_sf <- sf::st_as_sf(x, coords = c("lon", "lat"), crs = "WGS84") |>
    sf::st_transform(x, crs = crs)
  xy <- sf::st_coordinates(x_sf)
  xy <- mutate(as.data.frame(xy), X = X * mult, Y = Y * mult)
  x$X <- xy$X
  x$Y <- xy$Y
  list(dat_sf = x_sf, dat_sdmTMB = x)
}

.crs <- albers
# .csr <- conic
# .crs <- 32626
temp <- project_dat(dobs, .crs, mult = 0.01)
dobs_sf <- temp$dat_sf
dobs <- temp$dat_sdmTMB

temp <- project_dat(dlog, .crs, mult = 0.01)
dlog_sf <- temp$dat_sf
dlog <- temp$dat_sdmTMB
saveRDS(dlog_sf, "data-generated/dlog_sf.rds")
saveRDS(dobs_sf, "data-generated/dobs_sf.rds")
```

This is what the observed data (red) and log book data (grey) look like in space across all years combined.
We're also setting predictors to a standard value for visualization.

```{r plot-dat-map-log}
map_data <- rnaturalearth::ne_countries(
  scale = "small",
  returnclass = "sf"
)
map_data_cropped <- sf::st_crop(
  sf::st_make_valid(map_data),
  c(
    xmin = min(dlog$lon) - 20, ymin = min(dlog$lat) - 20,
    xmax = max(dlog$lon) + 20, ymax = max(dlog$lat) + 20
  )
)

gg_coord <- coord_sf(expand = FALSE, crs = "WGS84",
    xlim = c(range(dlog$lon) + c(-5, 5)),
    ylim = c(range(dlog$lat) + c(-5, 5))
  )

ggplot() +
  geom_sf(data = map_data_cropped, colour = "grey60", fill = "grey60") +
  geom_sf(data = dlog_sf, size = 0.2, alpha = 0.2) +
  geom_sf(data = dobs_sf, size = 0.2, alpha = 0.2, colour = "red") +
  gg_coord
```

Here the data are plotted by year with blue marlin by catch shown with colour.

```{r plot-dat-bycatch-year, echo=FALSE}
ggplot() +
  geom_sf(data = map_data_cropped) +
  geom_point(
    data = dobs, size = 0.3,
    mapping = aes(colour = bycatch, x = lon, y = lat)
  ) +
  gg_coord +
  facet_wrap(vars(year)) +
  scale_colour_viridis_c(trans = "log10")
```

We will transform several of our predictors to be factors. We could also do this with `as.factor()` on the fly in our models.

```{r factorize}
factorize <- function(x) {
  x$light_f <- as.factor(x$light)
  x$season_f <- as.factor(x$season)
  x$year_f <- as.factor(x$year)
  x
}
dobs <- factorize(dobs)
dlog <- factorize(dlog)
```

Here, we will come up with a grid visualizing model predictions and model components. We're just taking all unique log book locations across all years and creating a data frame that references those locations every year.

```{r grid}
grid <- select(dlog, X, Y) |> distinct()
nrow(grid)
grid_all <- purrr::map_dfr(sort(unique(dlog$year)), function(x) {
  bind_cols(tibble(year = x), grid)
})
grid_all <- left_join(grid_all, distinct(select(dlog, X, Y, lon, lat)))
grid_all$month <- 6
grid_all$light_f <- sort(unique(dobs$light_f))[1]
grid_all$season_f <- sort(unique(dobs$season_f))[1]
grid_all$hbf <- mean(dobs$hbf)
grid_all$year_f <- as.factor(grid_all$year)
nrow(grid_all)
if (!us_version) saveRDS(grid_all, file = "data-generated/grid_all.rds")
if (us_version) saveRDS(grid_all, file = "data-generated/grid_us.rds")
```

Now we will create a mesh that will be used in the SPDE approximation and bilinear interpolation along the triangles as part of the predictive process approach.
There are several ways we could create this mesh.
Here, we will create a mesh using a kmeans algorithm applied to the prediction (log-book) data set.
Often this mesh would be set up based on the observed data only, but we wanted to make sure we had appropriate mesh resolution in areas where there are log-book data but no observer coverage.
We select approximately 300 knots to achieve a reasonable balance of fine-scale accuracy and computational efficiency.
Model accuracy will not necessarily improve with increased mesh resolution beyond a certain point.

```{r mesh}
knots <- 300
mesh_all <- make_mesh(dlog, c("X", "Y"), n_knots = knots) # cutoff?

if (!us_version) mesh_all <- make_mesh(dlog, c("X", "Y"), cutoff = 5) # cutoff?
if (us_version) mesh_all <- make_mesh(dlog, c("X", "Y"), cutoff = 3) # cutoff?

mesh <- make_mesh(dobs, c("X", "Y"), mesh = mesh_all$mesh)
mesh$mesh$n
```

Here we will visualize the mesh. We could also create a simpler plot with the built in `plot()` method. 

```{r mesh-gg}
# plot(mesh)
ggplot() +
  geom_point(data = dobs, mapping = aes(X, Y), alpha = 0.2) +
  inlabru::gg(mesh$mesh) +
  coord_fixed() +
  theme_light()
```

```{r theme-light, echo=FALSE}
theme_set(theme_light())
```

```{r check-data, eval=FALSE, echo=FALSE}
table(dobs$light_f)
table(dobs$year_f)
table(dobs$season_f)

# make sure there aren't years with 100% or 0% observations:
group_by(dobs, year) |>
  summarise(pos_swo = mean(swo > 0), pos_bum = mean(bum > 0), n = n()) |>
  as.data.frame()
```


```{r filenames, echo=FALSE}
file_name_maker <- function(type, fleet, family, sp, st, knots, share_range) {
  f <- paste(type, fleet, family, "sp", sp, "st", st, "knots", knots, "share_range", share_range, sep = "-")
  here::here("data-generated", paste0(f, ".rds"))
}
fleet <- if (us_version) "us" else "all"
cache <- file_name_maker(type = 'cache', fleet = fleet, 
  family = c(""), 
  sp = 'on', st = params$spatiotemporal, knots = knots, share_range = params$share_range)
```

```{r cache-check, echo=FALSE}
files <- file_name_maker(type = 'fit', fleet = fleet, 
  family = c("nb2", "dg", "dl", "nb1"), 
  sp = params$spatial, st = params$spatiotemporal, knots = knots, share_range = params$share_range)
do_fit <- any(!file.exists(files))
message(do_fit)
```

Now we will fit our models.
We will structure the model to use:

- a series of main effect (first line: the `formula` argument)
- our observation data frame (`data`)
- our mesh (`mesh`)
- a NB2 negative binomial family (for this first model) (`family`)
- have an offset for log number of hooks (`offset`)
- include spatial random fields (`spatial = 'on'`)
- include independent spatiotemporal random fields (`spatiotemporal = 'iid'`)
- share or not share the range parameter between the spatial and spatiotemporal fields (`share_range = TRUE` or `share_range = FALSE`)
- include or not include anisotropy (`anisotropy`), which if included allows the spatial correlation to decay at different rates in different directions
- include additional optimization with a Newton optimizer (optional, helps convergence here)
- and to print out model fitting progress (`silent = FALSE`)

We will then use the `update()` method to refit our model with three additional families: delta-Gamma and delta-lognormal and NB1 negative binomial (where variance scales linearly with the mean instead of quadratically).

```{r cores}
# TMB::openmp(n = 1L, DLL = "sdmTMB") # optional; use 3 cores
```

```{r fit1, results='hide', eval=do_fit}
tictoc::tic()
fit_nb2 <- sdmTMB(
  bycatch ~ year_f + season_f + log(hbf) + light_f,
  data = dobs,
  mesh = mesh,
  family = nbinom2(),
  time = "year",
  offset = log(dobs$hooks),
  spatial = params$spatial,
  spatiotemporal = params$spatiotemporal,
  share_range = params$share_range,
  anisotropy = params$anisotropy,
  control = sdmTMBcontrol(newton_loops = 1),
  silent = FALSE
)
tictoc::toc()
fit_nb1 <- update(fit_nb2, family = nbinom1())

if (!us_version) {
fit_dg <- update(fit_nb2, family = delta_gamma())
fit_dl <- update(fit_nb2, family = delta_lognormal())
} else {
  fit_dl <- update(fit_nb2, family = delta_lognormal(), spatiotemporal = "off")
  fit_dg <- update(fit_nb2, family = delta_gamma(), spatiotemporal = "off")
}
```

```{r saverds, eval=do_fit, echo=FALSE}
saveRDS(fit_nb2, files[1])
saveRDS(fit_dg, files[2])
saveRDS(fit_dl, files[3])
saveRDS(fit_nb1, files[4])
# saveRDS(fit_dnb2, files[4])
```

```{r readrds, eval=!do_fit, echo=FALSE}
fit_nb2 <- readRDS(files[1])
fit_dg <- readRDS(files[2])
fit_dl <- readRDS(files[3])
fit_nb1 <- readRDS(files[4])
# fit_dnb2 <- readRDS(files[4])
```

```{r print-nb2, message=TRUE}
print(fit_nb2)
sanity(fit_nb2)
```

```{r print-nb1, message=TRUE}
print(fit_nb1)
sanity(fit_nb1)
```

```{r print-dg, message=TRUE}
print(fit_dg)
sanity(fit_dg)
```

```{r print-dl, message=TRUE}
print(fit_dl)
sanity(fit_dl)
```

The `sanity()` method runs some basic checks and here does not detect any obvious issues.

We can compare the AIC between the models. Here the delta-lognormal model has the lowest AIC.
However, we should take these with a grain of salt given the extensive random effects included.

A better assessment would be with cross validation with the `sdmTMB_cv()` function.
However, this would take considerable time and we do not perform that here.

```{r aic}
AIC(fit_nb2, fit_nb1, fit_dg, fit_dl) |> 
  arrange(AIC)
```

We can visualize the anisotropy:

```{r plot-aniso, out.width="60%", eval=params$anisotropy && params$spatiotemporal != "off"}
plot_anisotropy(fit_nb2)
plot_anisotropy(fit_nb1)
plot_anisotropy(fit_dg)
plot_anisotropy(fit_dl)
```

The ellipses indicate these spatial and spatiotemporal range parameters in any direction.
The range is the distance at which two data points are effectively independent (~0.13 correlation).
The ellipses are centered on 0 and the units are the units of the X and Y coordinates, which here are 100 km given the Albers projection.
In this case, we observe considerable  anisotropy with correlation decaying more quickly in an approximately  latitudinal direction compared to an approximately longitudinal direction.
We also estimated larger spatial ranges than spatiotemporal ranges (sold vs. dotted lines) and broader spatial correlation in the binomial model but broader spatiotemporal correlation in the positive model (green vs. orange ellipses).

```{r ggeffect, eval=FALSE, echo=FALSE}
# ggeffects::ggeffect(fit_nb2, terms = "light_f") |> plot()
# ggeffects::ggeffect(fit_nb2, terms = "season_f") |> plot()
# ggeffects::ggeffect(fit_nb2, terms = "year_f") |> plot()
# ggeffects::ggeffect(fit_nb2, terms = "hbf") |> plot()
```

We can look at several types of residuals.
The fast as to calculate or randomized quantile residuals with the parameters at their MLEs.
However, these residuals have known problems and can indicate problems even when there are none.

```{r quant-resids, out.width="40%", fig.width=5, fig.asp=1}
plot_resids <- function(x) {
  r <- residuals(x)
  qqnorm(r, asp = 1)
  qqline(r)
}
plot_resids(fit_nb2)
plot_resids(fit_nb1)
plot_resids(fit_dg)
plot_resids(fit_dl)
```

We could also use the DHARMa package to calculate simulated residuals:

```{r dharma, eval=FALSE}
plot_dharma_resids <- function(x) {
  s <- simulate(x, nsim = 200L)
  dharma_residuals(s, x)
}
plot_dharma_resids(fit_nb2)
plot_dharma_resids(fit_nb1)
plot_dharma_resids(fit_dg)
plot_dharma_resids(fit_dl)
```

Or, the most reliable option: calculate randomized quantal residuals with the random effects sampled with MCMC with Stan and the fixed effects fixed at their MLEs.
This will be quite slow and so we skip this here.

```{r mcmc-resids, eval=FALSE}
r_mcmc <- residuals(
  fit_nb2,
  type = "mle-mcmc",
  mcmc_warmup = 100, mcmc_iter = 200
)
```

We can visualize the residuals in space:

```{r spatial-residuals}
dobs$resid <- residuals(fit_dl)
dobs |>
  filter(year %in% c(1990, 2000, 2015)) |> # pick a few
  ggplot(aes(lon, lat, colour = resid)) +
  geom_point(size = 0.8, position = position_jitter(width = 0.25, height = 0.25)) +
  facet_wrap(~year) +
  scale_colour_gradient2() +
  coord_equal()
```

Next we will predict on our consistent grid; this is just for visualization.

```{r predict-on-grid}
p_grid <- predict(fit_dl, newdata = grid_all)
```

Here's a small function to help us make our plots:

```{r plot-map-func}
plot_map <- function(dat, column = est) {
  g <- ggplot(dat, aes(lon, lat, fill = {{ column }})) +
    geom_raster() +
    coord_fixed()
  if (length(unique(dat$year)) > 1) g <- g + facet_wrap(vars(year))
  g
}
```

This is the overall predicted bycatch in space over time.
For the delta models, this combines both components.

```{r plot-est, fig.asp=1}
plot_map(p_grid, plogis(est1) * exp(est2)) +
  scale_fill_viridis_c(trans = "sqrt")
```

Here we will zoom in on three arbitrary years so that they are easier to look at:

```{r plot-est-zoom}
p_grid |>
  filter(year %in% c(1990, 2000, 2015)) |> # pick a few
  plot_map(plogis(est1) * exp(est2)) +
  scale_fill_viridis_c(trans = "sqrt")
```

This is the delta model first component: the probability of observing any bycatch:

```{r plot-est1, fig.asp=1}
plot_map(p_grid, plogis(est1)) +
  scale_fill_viridis_c(option = "C")
```

This is the expected bycatch conditional on observing any bycatch:

```{r plot-est2, fig.asp=1}
plot_map(p_grid, exp(est2)) +
  scale_fill_viridis_c(option = "D", trans = "sqrt")
```

These are the spatial random effects from the binomial component of the delta model:

```{r plot-omega1, eval="omega_s1" %in% names(p_grid)}
p_grid |>
  filter(year == min(p_grid$year)) |> # pick any one
  plot_map(omega_s1) +
  scale_fill_gradient2()
```

These are the spatial random effects from the positive component of the delta model:

```{r plot-omega2, eval="omega_s2" %in% names(p_grid)}
p_grid |>
  filter(year == min(p_grid$year)) |> # pick any one
  plot_map(omega_s2) +
  scale_fill_gradient2()
```

These are the spatiotemporal random effects from the binomial component of the delta model:

```{r plot-eps1, eval="epsilon_st1" %in% names(p_grid), fig.asp=1}
p_grid |>
  plot_map(epsilon_st1) +
  scale_fill_gradient2()
```

These are the spatiotemporal random effects from the positive component of the delta model:

```{r plot-eps2, eval="epsilon_st2" %in% names(p_grid), fig.asp=1}
p_grid |>
  plot_map(epsilon_st2) +
  scale_fill_gradient2()
```

```{r check-sim-zero}
# s <- simulate(fit_nb2, nsim = 100L)
# mean(s == 0)
# mean(dobs$swo == 0)
```

The fastest way to visualize uncertainty in space is to simulate from the joint precision matrix of the parameter space and calculate a measure of uncertainty from those draws.
Here, we will calculate the coefficient of variation and plot it for an arbitrary year since the uncertainty looks relatively similar across years.

```{r plot-cv}
pred <- predict(fit_dl, newdata = grid_all, nsim = 100)
grid_all$cv <- apply(pred, 1, function(x) sd(exp(x)) / mean(exp(x)))

filter(grid_all, year %in% c(2000)) |> # pick one; very similar
  ggplot(aes(lon, lat, fill = cv)) +
  geom_raster() +
  scale_fill_viridis_c(option = "A", trans = "log10") +
  coord_equal() +
  facet_wrap(~year)
```

```{r cache-check-index, echo=FALSE}
index_file <- file_name_maker(type = 'index', fleet = fleet,
  family = c("all"), 
  sp = params$spatial, st = params$spatiotemporal, knots = knots, share_range = params$share_range)
do_calc_index <- !file.exists(index_file)
```

Next we will combine our models into a list so we can iterate over the list when we calculate the predicted bycatch total.

```{r models-list}
models <- list(fit_nb2, fit_dg, fit_dl, fit_nb1)
names(models) <- c("NB2", "Delta-lognormal", "Delta-Gamma", "NB1")
```

Here we create a small function that predicts from our models onto the logbook data and passes those predictions through the `get_index()` function in sdmTMB.
`get_index()` sums the predictions within each year and calculates uncertainty on those predictions using the generalized delta method and a bias correction that accounts for the non-linear (exp) transformation of the random effects.

```{r calc-indices, eval=do_calc_index}
calculate_index <- function(x) {
  pred <- predict(x,
    newdata = dlog,
    offset = log(dlog$hooks), return_tmb_object = TRUE
  )
  ind <- sdmTMB::get_index(pred, bias_correct = TRUE)
  ind
}
tictoc::tic()
ind <- purrr::map_dfr(models, calculate_index, .id = "model")
tictoc::toc()
```

```{r saverds-index, eval=do_calc_index, echo=FALSE}
saveRDS(ind, index_file)
```

```{r readrds-index, eval=!do_calc_index, echo=FALSE}
ind <- readRDS(index_file)
```

Here we will calculate the true total bycatch and combine that data with our calculated indexes.

```{r plot-indices-prep, echo=TRUE}
true <- group_by(dlog, year) |>
  summarize(swo = sum(swo), bum = sum(bum),
    type = "simulated", model = "simulated")
true$est <- true[[params$species]]

temp <- ind |>
  mutate(type = "predicted")

temp <- temp |> group_by(model) |> 
  mutate(
    any_not_converged = 
      sum(is.infinite(upr)) > 0 | sum(lwr == 0) > 0
    ) |> 
  filter(!any_not_converged)

temp <- temp |>
  bind_rows(true)
```

Plot the predicted and actual totals:

```{r plot-indices, echo=FALSE}
cols <- RColorBrewer::brewer.pal(length(models), "Dark2")
names(cols) <- names(models)
cols <- c(cols, c("simulated" = "#000000"))

gg_scales <- list(scale_color_manual(values = cols),
  scale_fill_manual(values = cols),
  scale_linetype_manual(values = c("predicted" = 1, "simulated" = 2)),
  coord_cartesian(ylim = c(0, NA)))

gg_labs <- list(ylab("Bycatch total"),
  xlab("Year"),
  labs(lty = "Type", colour = "Model", fill = "Model"))

temp |>
  filter(model != "simulated") |> 
  ggplot(aes(year, est,
    ymin = lwr, ymax = upr, lty = type,
    colour = model, fill = model
  )) +
  geom_ribbon(alpha = 0.5, colour = NA) +
  geom_line() +
  geom_line(data = true |> select(-model), lty = 2, colour = "black", 
    inherit.aes = FALSE, mapping = aes(year, est)) +
  gg_scales +
  gg_labs +
  facet_wrap(vars(model), scales = "free_y", nrow = 2)

temp |>
  ggplot(aes(year, est,
    ymin = lwr, ymax = upr, lty = type,
    colour = model, fill = model
  )) +
  geom_ribbon(alpha = 0.5, colour = NA) +
  geom_line() +
  gg_scales +
  gg_labs
```

And calculate the coverage from our 95% confidence intervals.

```{r coverage-raw}
left_join(ind, select(true, year, true = est)) |>
  mutate(
    covered = true < upr & true > lwr
  ) |>
  group_by(model) |>
  summarise(coverage = mean(covered)) |> 
  arrange(-coverage)
```

```{r coverage50, eval=FALSE, echo=FALSE}
left_join(ind, select(true, year, true = est)) |>
  mutate(upr = exp(log_est + qnorm(0.75) * se), lwr = exp(log_est + qnorm(0.25) * se)) |> 
  mutate(
    covered = true < upr & true > lwr
  ) |>
  group_by(model) |>
  summarise(coverage = mean(covered))
```

```{r centered, eval=FALSE, echo=FALSE}
temp <- left_join(ind, select(true, year, true = est)) |>
  group_by(model) |>
  mutate(
    geomean = exp(mean(log(est))),
    true_geomean = exp(mean(log(true)))
  ) |>
  mutate(
    upr = upr / geomean,
    lwr = lwr / geomean,
    est = est / geomean,
    true = true / true_geomean
  )

temp |>
  ggplot(aes(year, est, colour = model, fill = model)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.5, colour = NA) +
  geom_line(aes(y = true), lty = 2) +
  gg_labs +
  gg_scales +
  ylab("Centered bycatch")

# centered coverage
temp |>
  group_by(model) |>
  mutate(covered = true < upr & true > lwr) |>
  group_by(model) |>
  summarise(coverage = mean(covered))
```
